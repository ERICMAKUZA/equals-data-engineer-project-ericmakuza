{
	"jobConfig": {
		"name": "transform-to-star-schema",
		"description": "",
		"role": "arn:aws:iam::114894399234:role/Glue-Project-Role",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "transform-to-star-schema.py",
		"scriptLocation": "s3://aws-glue-assets-114894399234-eu-north-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-08-06T09:22:52.634Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-114894399234-eu-north-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-114894399234-eu-north-1/sparkHistoryLogs/",
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "\r\nimport sys\r\nfrom awsglue.transforms import *\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col, year, month, dayofmonth, date_format, to_date\r\n# ✅ Import new types for manual schema definition\r\nfrom pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\r\n\r\n# --- 1. INITIALIZATION ---\r\nsc = SparkContext.getOrCreate()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\n\r\n# --- CONFIGURATION ---\r\nsource_database = \"financial_data_db\" \r\ns3_output_path = \"s3://equals-project-data-2025/analytics/\" #\r\ns3_events_path = \"s3://equals-project-data-2025/raw/financialdb/transaction_events/\" # ⚠️ Replace with your S3 bucket name\r\n\r\n# --- 2. LOAD DATA ---\r\nprint(\"Loading raw data...\")\r\n\r\n# Load the clean PostgreSQL data from the Glue Catalog\r\ncustomers_df = glueContext.create_dynamic_frame.from_catalog(database=source_database, table_name=\"customers\").toDF()\r\naccounts_df = glueContext.create_dynamic_frame.from_catalog(database=source_database, table_name=\"accounts\").toDF()\r\ntransactions_df = glueContext.create_dynamic_frame.from_catalog(database=source_database, table_name=\"transactions\").toDF()\r\n\r\n# ✅ FINAL FIX: Read the problematic DocumentDB data directly from S3 and apply schema manually\r\n\r\n# Define the correct schema for the events data\r\nevents_schema = StructType([\r\n    StructField(\"transaction_id\", LongType(), True),\r\n    StructField(\"device_type\", StringType(), True),\r\n    StructField(\"ip_address\", StringType(), True),\r\n    StructField(\"geolocation\", StructType([\r\n        StructField(\"country\", StringType(), True),\r\n        StructField(\"city\", StringType(), True)\r\n    ]), True),\r\n    StructField(\"fraud_score\", DoubleType(), True)\r\n])\r\n\r\n# Read the Parquet files from S3 using the manually defined schema\r\nprint(f\"Reading events data directly from {s3_events_path}\")\r\nevents_df = spark.read.schema(events_schema).parquet(s3_events_path)\r\nprint(\"Events data successfully loaded from S3.\")\r\n\r\n\r\n# --- 3. CREATE DIMENSION TABLES ---\r\n# (This section remains unchanged)\r\n\r\nprint(\"Creating dim_customers...\")\r\ndim_customers = customers_df.select(\r\n    col(\"customer_id\").alias(\"customer_key\"), col(\"name\"), col(\"email\"), col(\"address\"), col(\"phone\")\r\n)\r\ndim_customers.write.mode(\"overwrite\").parquet(f\"{s3_output_path}/dim_customers/\")\r\nprint(\"dim_customers successfully written to S3.\")\r\n\r\nprint(\"Creating dim_accounts...\")\r\ndim_accounts = accounts_df.select(\r\n    col(\"account_id\").alias(\"account_key\"), col(\"customer_id\").alias(\"customer_key\"), col(\"account_type\"), col(\"opened_at\"), col(\"balance\")\r\n)\r\ndim_accounts.write.mode(\"overwrite\").parquet(f\"{s3_output_path}/dim_accounts/\")\r\nprint(\"dim_accounts successfully written to S3.\")\r\n\r\nprint(\"Creating dim_dates...\")\r\ndim_dates = transactions_df.select(transactions_df[\"timestamp\"]).distinct().select(\r\n    to_date(col(\"timestamp\")).alias(\"date\")\r\n).distinct()\r\ndim_dates = dim_dates.select(\r\n    col(\"date\"), date_format(col(\"date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_key\"),\r\n    year(col(\"date\")).alias(\"year\"), month(col(\"date\")).alias(\"month\"), dayofmonth(col(\"date\")).alias(\"day\")\r\n)\r\ndim_dates.write.mode(\"overwrite\").parquet(f\"{s3_output_path}/dim_dates/\")\r\nprint(\"dim_dates successfully written to S3.\")\r\n\r\n\r\n# --- 4. CREATE FACT TABLE ---\r\n# (This section remains unchanged)\r\n\r\nprint(\"Creating fact_transactions...\")\r\nfacts = transactions_df.join(accounts_df, \"account_id\")\r\nfacts = facts.join(events_df, \"transaction_id\", \"left_outer\")\r\nfacts = facts.withColumn(\"date_key\", date_format(to_date(transactions_df[\"timestamp\"]), \"yyyyMMdd\").cast(\"int\"))\r\n\r\nfact_transactions = facts.select(\r\n    transactions_df[\"transaction_id\"], col(\"date_key\"),\r\n    accounts_df[\"account_id\"].alias(\"account_key\"), accounts_df[\"customer_id\"].alias(\"customer_key\"),\r\n    transactions_df[\"amount\"], transactions_df[\"transaction_type\"],\r\n    events_df[\"device_type\"], events_df[\"fraud_score\"]\r\n)\r\nfact_transactions.write.mode(\"overwrite\").parquet(f\"{s3_output_path}/fact_transactions/\")\r\nprint(\"fact_transactions successfully written to S3.\")\r\n\r\njob.commit()"
}